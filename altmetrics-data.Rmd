---
geometry: margin=1.5in
output:
  pdf_document:
    fig_caption: yes
tables: yes
---
```{r eval = FALSE, echo=FALSE, message=FALSE, warning=FALSE}
fig.height = 2
fig.width = 1.5 * fig.height
font_size = 8

knitr::opts_chunk$set(dev = 'tikz',
			   fig.height = fig.height, fig.width = fig.width,
			   fig.align='center', fig.pos = 't',
			   sanitize = TRUE,
			   echo = FALSE,
			   warning = FALSE, message = FALSE,
			   cache = FALSE
			   )
options(digits = 2, xtable.comment = FALSE, xtable.caption.placement = 'top', 
		xtable.include.rownames = FALSE)
```

## Data ##

Bibliometrics, like other opportunistic uses of independently-cultivated data, depends on stable identifiers that can track individual research targets — such as research publications — across datasets from multiple sources.  DOI, or the digital object identifier, has emerged as a major standard identifier for research publications.  Other identifiers, such as PubMed's internal identifier, are common, but not as widely-used as DOI [@Kraker2015].  Altmetrics' API accepts queries in terms of both DOIs and PubMed IDs.  

The first step in any bibliometrics analysis, then, is determining the DOI for each publication of interest.  For an analysis of CSS publications, ideally, these DOIs could be identified automatically along with the publications of interest, using a database that (a) includes DOIs in the publication metadata, (b) permits a search by EPA research program, and (c) can export machine-readable search results.  As far as I have been able to tell, EPA does not have any general publications database that satisfies all three desiderata.  The "public-facing" version of Science Inventory (<https://cfpub.epa.gov/si/>) does not appear to satisfy any of these three requirements. The "internal application" version of Science Inventory (<https://cfext.epa.gov/si/SciInv/stmProtoLogin.cfm>) requires a separate registration for access; when I attempted to register a new account, the system generated errors that could not be resolved.  

STICS [Science and Technical Information Clearance System] is designed to support the clearance and approval of research products before they are submitted to a journal for publication.  STICS satisfies criteria (b) and (c), and thus can export a list of all research products associated with CSS.  However, presumably because it is designed for use only pre-publication, STICS does not include DOIs. 
After examining the metadata outputs from STICS, I decided that the most efficient way to identify DOIs corresponding to STICS records would be to search for matching titles in Scopus, a large database of research publications similar to Web of Science or PubMed.  [For a comparative analysis of Scopus, Web of Science, PubMed, and Google Scholar for bibliometrics projects, see @Mingers2015.]  Python scripts were prepared to conduct both "quoted" and "unquoted" searches for each research product title.  A quoted search matches the title as a complete phrase; for example, a quoted search for the title "Recent Work in High-Throughput Toxicology" would not match "Recent Work for High-Throughput Toxicology."  An unquoted search matches the individual terms; for example, "Recent Work in High-Throughput Toxicology" *would* match with "Recent Work for High-Throughput Toxicology" (assuming that the search system ignored very common words such as "for" and "in").  An unquoted search is useful for catching publications for which the title had been changed sometime during the review process, or for handling encoding errors (such the title stored in STICS as `A Framework for &quot;Fit for Purpose&quot; Dose Response Assessment`); however, an unquoted search is obviously more likely to return incorrect matches.  

The results of these STICS-derived searches were compared with two manually-curated databases of publications:  an EndNote database sporadically updated by CSS staff and a database of NCCT publications curated by Monica Linnenbrink.  Both of the latter two databases include DOIs, but neither is intended to be a comprehensive collection of all CSS publications.  In particular, the NCCT database includes several publications that predate the creation of the CSS national research program in 2011; these publications were excluded from analysis.  Otherwise, the combined results from all four database/searches were used in the analysis in the next section.  

```{r, eval = FALSE}
## Run the Python scripts to extract cleaner data files from the db outputs
system2('python3', 'publications\\ data/extract_ids_CSS.py')
system2('python3', 'publications\\ data/extract_ids_NCCT.py')
system3('python3', 'publications\\ data/stics_to_doi.py')
```

```{r setup_data, cache = FALSE}
library(cowplot)
	theme_set(theme_cowplot(font_size = font_size))
library(dplyr)
library(knitr)
library(rjson)
library(reshape2)
library(xtable)

folder = 'publications data/scraped pubs/'

ids_css = fromJSON(file = paste(folder, 'ids_css.json', sep = '')) %>% unique
ids_css = data.frame(doi = ids_css, endnote = TRUE, stringsAsFactors = FALSE)
## Replace some mis-stored DOIs
ids_css$doi = gsub('doi:', '', ids_css$doi)
ids_css$doi = gsub('http://dx.doi.org/', '', ids_css$doi)

ids_ncct = read.csv(paste(folder, 'ids_ncct.csv', sep = ''), 
					stringsAsFactors = FALSE)
ids_ncct = ids_ncct[ids_ncct$'Pub.Yr' >= 2011,]$DOI
ids_ncct = data.frame(doi = ids_ncct, ncct = TRUE, stringsAsFactors = FALSE)

stics_q = read.csv(paste(folder, 'stics_q.csv', sep = ''), 
				   stringsAsFactors = FALSE)$doi
stics_q = unlist(regmatches(stics_q, gregexpr("'[^']*'", stics_q)))
stics_q = gsub("'", "", stics_q)
stics_q = data.frame(doi = stics_q, stics.q = TRUE, stringsAsFactors = FALSE)

stics_uq = read.csv(paste(folder, 'stics_uq.csv', sep = ''), 
					stringsAsFactors = FALSE)$doi
stics_uq = unlist(regmatches(stics_uq, gregexpr("'[^']*'", stics_uq)))
stics_uq = gsub("'", "", stics_uq)
stics_uq = data.frame(doi = stics_uq, stics.uq = TRUE, stringsAsFactors = FALSE)

dataf = full_join(ids_css, ids_ncct) %>% full_join(stics_q) %>% 
	full_join(stics_uq) %>% 
	filter(doi != '')
dataf[is.na(dataf)] = FALSE
## Remove duplicates
dataf = unique(dataf)

db_names = c('EndNote', 'NCCT', 'STICS\n(Quoted)', 'STICS\n(Unquoted)')
```

Manual inspection of the results identified several apparent false matches; publications with no EPA-affiliated authors were discarded. 

```{r remove_nonepa}
# write.csv({dataf %>% filter(!(stics.q & stics.uq)) %>% .[['doi']]},
# 			file = 'publications data/scraped pubs/poss nonepa.csv')
# stop()
nonepa = read.csv(paste(folder, 'poss nonepa.csv', sep = '')) %>%
	filter(!epa|is.na(epa)) %>% .[['doi']] %>% as.character
# dataf %>% filter(doi %in% nonepa) %>% select(stics.q, stics.uq, endnote) %>% table
dataf = dataf %>% filter(!(doi %in% nonepa))
```

```{r results='asis'}
dataf %>% select(endnote:stics.uq) %>% summarize_each(funs = 'sum') %>% 
	setNames(gsub('\n', ' ', db_names)) %>%
	xtable(format = 'latex',
		  caption = 'Publications with DOIs found in each database/search 
		  				\\label{tab.test}')
```

Table \ref{tab.test} gives an overview of the number of DOIs included in each database/search.  Combining the results of all four database/searches yields a total of `r length(unique(dataf$doi))` distinct DOIs.  Both of the two STICS searches include a majority of these DOIs; the EndNote database contains somewhat fewer than half; and the NCCT database contains just over 10% of all DOIs.  

```{r papers_dbs, fig.width = 1.75 * fig.width, fig.cap = 'Distribution of individual papers across the four database/searches. \\textbf{A}: Each paper is represented by a single row; a red cell indicates that the given paper is included in the given database/search. \\textbf{B}: Each paper is represented by an unbroken line across the parallel coordinates. Y-axis indicates whether the given paper is included in the given database/search. \\label{fig.papers_dbs}'}
db_pubs_1 = ggplot(data = {dataf %>% melt(id.vars = 'doi', 
										  variable.name = 'database')},
	   aes(doi, database, fill = value)) +
	geom_tile() + 
	scale_x_discrete(labels = NULL) +
	scale_y_discrete(labels = db_names) +
	scale_fill_manual(values = c(NA, 'red'), guide = FALSE) + coord_flip()

db_pubs_2 = ggplot(data = {dataf %>% melt(id.vars = 'doi', 
										  variable.name = 'database',
										  value.name = 'included')}, 
	aes(x = database, y = included, group = doi)) + 
	geom_line(position = position_jitter(height = .5), alpha = .05) +
	scale_x_discrete(labels = db_names)

plot_grid(db_pubs_1, db_pubs_2, ncol = 2, labels = 'AUTO', align = 'v')
```

Figure \ref{fig.papers_dbs} shows the distribution of every individual paper across the four database/searches, as lit cells in a heatmap and as lines across parallel coordinates.  These plots indicate that the two STICS searches include almost exactly the same publications, and that there is *[relatively little overlap between NCCT and the other database/searches]*.  Tables \ref{tab.dist} and \ref{tab.STICSconcordance} make these same points quantitatively.  

```{r}
# dataf %>% select(endnote:stics.uq) %>% summary
dist = dataf %>% group_by(stics.q, stics.uq, endnote, ncct) %>% 
	summarize(n = n())
dist[dist == FALSE] = ''
dist[dist == TRUE] = 'X'
dist %>% 
	setNames(c('STICS (Quoted)', 'STICS (Unquoted)', 'EndNote', 
						'NCCT', 'n')) %>%
	xtable(align = c(rep('c', 5), 'r'),
		format = 'latex',
		caption = 'Distribution of papers across the four database/searches
					\\label{tab.dist}')
dist = dataf %>% group_by(stics.q, stics.uq) %>% summarize(n = n())
dist[dist == FALSE] = ''
dist[dist == TRUE] = 'X'
dist %>% 
	setNames(c('STICS (Quoted)', 'STICS (Unquoted)', 'n')) %>%
	xtable(
		align = c('c', 'c', 'c', 'r'),
		format = 'latex',
		caption = 'Concordance between quoted and unquoted STICS search results
						\\label{tab.STICSconcordance}')
```

```{r export, eval=FALSE}
write(toJSON(dataf$doi), paste(folder, 'combined dois.json', sep = ''))
```

