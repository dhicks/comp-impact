---
output:
  pdf_document:
    fig_caption: yes
    keep_tex: true
  html_document: default
geometry: margin=1.5in
tables: yes
---
```{r eval = FALSE, echo=FALSE, message=FALSE, warning=FALSE}
fig.height = 2.5
fig.width = 1.5 * fig.height
font_size = 8

knitr::opts_chunk$set(dev = 'tikz',
			   fig.height = fig.height, fig.width = fig.width,
			   fig.align='center', fig.pos = 't',
			   sanitize = TRUE,
			   echo = TRUE,
			   warning = TRUE, message = TRUE
			   )
options(digits = 2, xtable.comment = FALSE, xtable.caption.placement = 'top')
```

# Study 2: Bloomberg BNA #

Media analysis provides another approach to assessing the impact of a public-interest scientific research program.  Traditional methods of media analysis require manual review of articles and other documents.  While these methods can provide a rich and nuanced understanding of the way scientific research is represented to the public, and changes in this representation over time, they may be too resource-intensive in some cases.  In such cases, text-mining tools — computational tools for quantitative analysis of large bodies of text — may be used to produce quick — but potentially "thin" or un-nuanced — analyses and suggest refined targets for analysis using classical techniques  

This section applies text mining tools to analyze a large dataset of trade media coverage of regulatory toxicology.  This study addresses two questions: 

1. How has coverage of the ToxCast program, CSS, and high-throughput toxicology [HTT] more generally, changed over time?  
2. When HTT is covered, how is it represented? 



## Data ##

```{r parse-xml, eval = FALSE}
## BNA's API returns XML files with match metadata (title, publication date, 
## a short description, etc.) and a link to the fulltext.  
## This script parses the XML files into a combined CSV.  
source('BNA data/parse_bna.R')
## And this one uses the list of links to download fulltext
source('BNA data/scrape_article_text.R')
```

```{r setup_bna, cache = FALSE}
library(cowplot)
	theme_set(theme_cowplot(font_size = font_size))
library(dplyr)
library(GGally)
library(knitr)
library(latex2exp)
library(lme4)
library(lubridate)
library(reshape2)
library(syuzhet)
library(tm)
library(xtable)
```
```{r load_data}
## Load Bloomberg BNA data
dataf = read.csv('BNA data/CSV/BNA.text.csv', stringsAsFactors = FALSE)
dataf = dataf %>% transmute(date = {dmy_hms(pubDate) %>% 
										floor_date(unit = 'day')}, 
						title = title,
						byline = byline,
						body = text) %>%
	filter(date > ymd('2000-01-01'),
		   !grepl(paste('FEDERAL REGISTER',
		   			 'Comment Deadlines', 
		   			 'Compiled Information', 
		   			 "TODAY'S EVENTS",
		   			 'MEETINGS AND COURSES',
		   			 'CORRECTION', 
		   			 sep = '|'),
		   	   title))
```

With assistantance from Bloomberg BNA staff, BNA's web API was used to retrieve all articles using the word "toxicology" published since 2000.  Article metadata were retrieved in XML files; custom R scripts were used to parse and combine these XML files, then retrieve text and byline for each article.  Manual inspection of the resulting article set found several hundred items that were short summaries of *Federal Register* notices, announcements of public comment periods, lists of events, and other article types that were judged to be irrelevant; all of these article types were removed.    

Because BNA's own database was queried directly using the API, the dataset can be considered complete and error-free.  One remaining potential source of data error is the inclusion of repeated or duplicate articles.  Manual review of titles and a programmatic check indicated no duplicate articles.  
```{r dupes_check, eval = FALSE}
## Check titles
dataf$title[duplicated(dataf$title)]
## Check article text
dataf$body[duplicated(dataf$body)]
```

## Methods ##

### "ToxCast" Scores ###

```{r text_mining_setup}
## Text-mining prep
## Build the corpus
corpus = VCorpus(DataframeSource(dataf), 
				 readerControl = list(reader = readTabular(mapping = 
				 								list(date = 'date', 
				 									 title = 'title',
				 									 content = 'body')), 
				 					 language = 'en'))

## tm's removePunctuation doesn't replace punctuation with whitespace
remove_punct = function (x) {return(gsub('[^-[:^punct:]]', ' ', x, perl = TRUE))}
## Assume it's safe to drop non-ASCII characters
only_ascii = function (x) {return(gsub('[^[:ascii:]]', '', x, perl = TRUE))}
corpus = corpus %>% 
	tm_map(content_transformer(tolower), mc.cores = 1) %>% 
	tm_map(removeWords, stopwords("english")) %>% 
	tm_map(content_transformer(remove_punct)) %>% 
	tm_map(content_transformer(only_ascii)) %>%
	#tm_map(removePunctuation, preserve_intra_word_dashes = TRUE)  %>%
	tm_map(removeNumbers) %>% 
	## Stemming didn't seem to work well
	#tm_map(stemDocument) %>% 
	tm_map(stripWhitespace)

## Document-term matrix and frequencies
termdoc = TermDocumentMatrix(corpus)
```

To address research question 1, the vocabulary used in the articles was first normalized.  Individual terms were converted to lowercase; punctuation was removed (except for interword hyphens, as in "high-throughput"), along with numbers, special symbols, and stopwords (words so common that they can confound text mining, such as "the"). 

```{r build_term_sets}
## Extract the toxcast row
toxcast = as.matrix(termdoc)['toxcast',]

## Calculate distances from toxcast for each term
## L1 norm
#distances = apply(as.matrix(termdoc), 1, function (x) sum(abs(x - toxcast)))
## Jaccard distance
distances = apply(as.matrix(termdoc), 1,
				  function (x) 1 - sum((x * toxcast) > 0) / 
				  					sum((x + toxcast) > 0))

## Plot the distribution of distances
# ggplot(data = data.frame(dist = sort(distances), x = 1:length(distances)), 
# 	   aes(x, dist)) + geom_line() + 
# 	geom_vline(xintercept = c(10, 100, 1000), color = 'blue') +
# 	ylab('Jaccard distance from ``toxcast"') +
# 	xlab('terms') +
# 	scale_x_log10(breaks = 10^(0:4))

## Get the 10, 100, and 1000 terms closest to toxcast in the corpus
toxcast10 = names(sort(distances)[1:10])
toxcast100 = names(sort(distances)[1:100])
toxcast1k = names(sort(distances)[1:1000])
rm(toxcast, distances)
```

```{r calculate_scores}
## Calculate scores for each document
scores = lapply(
	list(toxcast = 'toxcast', 
		 toxcast10 = toxcast10,
		 toxcast100 = toxcast100, 
		 toxcast1k = toxcast1k), 
	function(x) tm_term_score(termdoc, x)) %>% 
	as.data.frame
dataf = dataf %>% mutate(length = {termdoc %>% as.matrix %>% colSums}) %>%
	cbind(scores)
rm(scores)

## Calculate monthly total scores, no. articles, and total length
thresh_date = '2005-01-01'

dataf$month = dataf$date %>% floor_date(unit = 'month')
scores_month = dataf %>% group_by(month) %>%
	select(toxcast:toxcast1k) %>% summarize_each('sum') %>%
	right_join({dataf %>% group_by(month) %>% 
			summarize(n_articles = n(), tot_length = sum(length))})
scores_month = scores_month %>% 
	melt(id.vars = c('month', 'n_articles', 'tot_length'), 
		 measure.vars = c('toxcast', 'toxcast10', 'toxcast100', 'toxcast1k'),
		 variable.name = 'terms', value.name = 'score') %>%
	mutate(## For precision reasons, rescale `month` to number of years 
		   ## (365 days) relative to the threshold date
		   month.rs = difftime(month, thresh_date, unit = 'days')/365, 
		   ## lmer doesn't like that tot_length varies over a few orders 
		   ##   of magnitude
		   tot_length.rs = as.vector(scale(tot_length)))
```

To examine coverage of CSS within this corpus, the analysis focused on the (normalized) term "toxcast" and associated terms, meaning terms that tend to appear in the same documents as "toxcast," as well as sets of the 10, 100, and 1000 terms most strongly associated with "toxcast".  "Scores," occurrence frequencies across each of the four sets of terms, were calculated for each document, as both raw totals and normalized by document length.  Finally, scores were aggregated by month and linear trends over time were examined.  For the analysis presented here, distance calculations include February 2000.  For an analysis of the robustness of these calculations, see the appendix.  


### Sentiment Analysis ###

Sentiment analysis uses manually-prepared reference lists of emotionally-laden terms to estimate the emotional valence of texts.  In the particular technique used here, each text is assigned an emotional valence score — as "positive" or "fearful," say — based on the occurrence of individual words that human curators have judged to be "positive" or "fearful" [@Mohammad2013].  While this technique is obviously useful for addressing research question 2, concerning the way HTT is represented in the trade media, its results should not be over-interpreted.  Specifically, the results depend heavily on the content of the reference lists; do not take into account sentence structure; and are easily confounded by irony and other complex rhetorical constructions. 

This sentiment analysis tool was applied to every article in the corpus with a non-zero toxcast100 score.  The sentiment analysis tool estimates scores for ten emotional valences: "anger," "anticipation," "disgust," "fear," "joy," "sadness," "surprise," "trust," "negative," and "positive." Both raw (total frequency) and normalized (per 1,000 words in the article) sentiment scores were calculated.  

```{r sentiment-analysis-prep}
toxcast_stories = dataf %>% filter(toxcast100 > 0) %>% mutate(id = row_number())
toxcast_stories = cbind(toxcast_stories, get_nrc_sentiment(toxcast_stories$body))
toxcast_stories$length = sapply(toxcast_stories$body, 
							   function(x) length(get_tokens(x)), USE.NAMES = TRUE)

## Melt into a long dataframe for use with ggplot
toxcast_stories_m = toxcast_stories %>% 
	select(id, date, month, length, anger:positive) %>% 
	melt(id.vars = c('id', 'date', 'month', 'length')) %>%
	## It's easiest to calculate length-normalized scores here
	mutate(value.norm = value / length * 1000)
## Cast the length-normalized valence scores into the same shape as toxcast_stories
toxcast_stories = toxcast_stories_m %>% 
	dcast(id ~ variable, value.var = 'value.norm') %>% 
	right_join(toxcast_stories, by = 'id')
```


## Results ##

Publication date, title, byline, and full text were obtained for `r nrow(dataf)` articles. After normalization, the vocabulary included `r termdoc$nrow` distinct terms and a total of $`r dataf %>% .[['length']] %>% sum`$ tokens (word-instances).  See figure \ref{fig.monthly}. The large spike in total length in February 2000 is due to five long, high-level EPA policy documents apparently republished by BNA.  Consequently, articles from February 2000 are generally excluded from the analyses below.  After excluding February 2000, the corpus contained 
$`r dataf %>% filter(month != ymd('2000-02-01')) %>% .[['length']] %>% sum`$ 
tokens.  

```{r articles_time, fig.width = 1.5*fig.width, fig.height = 1.5*fig.height, fig.cap = 'Overview of the BNA dataset. \\textbf{A}: Number of articles published each month. \\textbf{B} and \\textbf{C}: Total length of articles per month (total token acount after normalization).  \\textbf{C} omits months with more than 5,000 total tokens. \\label{fig.monthly}'}
month_n_art_plot = ggplot(data = dataf, aes(x = month)) + 
	geom_line(stat = 'count', color = 'blue') + 
	xlab('month') + ylab('no. articles')
month_tot_len_plot = ggplot(data = dataf, aes(x = month, y = length)) +
	geom_line(stat = 'sum', size = .25, color = 'red') + 
	ylab('total length')
length_plot = plot_grid(month_tot_len_plot, month_tot_len_plot + ylim(0, 5000), 
		  nrow = 1, labels = c('B', 'C'), hjust = 0)
plot_grid(month_n_art_plot, length_plot, nrow = 2, labels = c('A', ''))
```

### "ToxCast" Scores ###

Table \ref{tab.toxcast100} shows the 100 terms most closely associated with "toxcast" in the dataset.  Figure \ref{fig.monthly_scores} shows monthly total ToxCast scores for the term "toxcast" by itself and the 10, 100, and 1000 terms most closely associated with "toxcast," excluding February 2000. In the remainder of this report, these sets of terms and scores are referred to as "toxcast10," "toxcast100," and "toxcast1000." 

```{r toxcast100, results='asis'}
matrix(sort(toxcast100), nrow = 5) %>% t %>%
	as.data.frame() %>%
	setNames(rep('', 5)) %>%
	xtable(caption = 'ToxCast 100 terms, normalized \\label{tab.toxcast100}', 
		  format = 'latex') %>%
	print(hline.after = NULL)
```
```{r plot_monthly_scores, fig.width = 1.5*fig.width, fig.height = 1.5*fig.height, fig.cap = 'ToxCast scores. Scores are calculated as monthly total occurrences of "toxcast" and its 10, 100, or 1000 most closely-associated terms. Blue lines indicate linear regressions before and after January 2005. Articles from February 2000 are excluded from this plot. \\label{fig.monthly_scores}'}
## Plot the results
ggplot(data = {scores_month %>% filter(month != ymd('2000-02-01'))}, 
	   aes(month, score)) + 
	geom_line() + 
	facet_wrap(~ terms, scale = 'free_y') +
	stat_smooth(data = function(x) filter(x, month < thresh_date), method = 'lm',
				se = TRUE) + 
	stat_smooth(data = function(x) filter(x, month >= thresh_date), method = 'lm',
				se = TRUE)
```
```{r monthly_scores_control, fig.width = 1.5*fig.width, fig.height = 1.5*fig.height, fig.cap = 'ToxCast scores after adjusting for monthly article and word totals.  Articles from February 2000 are excluded from this plot. \\label{fig.monthly_scores_control}'}
## Use a regression of scores on the number of articles and tot. article length
##   (rescaled) to analyze causes of changes over time
re_fit = lmer(data = {scores_month %>% filter(month != ymd('2000-02-01'))}, 
			score ~ 1 + (tot_length.rs + n_articles | terms))
scores_month = scores_month %>% mutate(re_predict = predict(re_fit, scores_month), 
						re_resid = score - re_predict)
ggplot(data = {scores_month %>% filter(month != ymd('2000-02-01'))}, 
	   aes(month, re_resid)) + 
	geom_line() + 
	facet_wrap(~ terms, scale = 'free_y') +
	stat_smooth(data = function(x) filter(x, month < thresh_date), method = 'lm',
				se = TRUE) + 
	stat_smooth(data = function(x) filter(x, month >= thresh_date), method = 'lm',
				se = TRUE) +
	ylab('adjusted score')
```

The first instance of "toxcast" was on 
`r scores_month %>% filter(terms == 'toxcast', score > 0) %>% .[['month']] %>% min %>% format(format = '%B %e, %Y')
`.  The plot suggests that, after January 2005, the term "toxcast" occurred more frequently but roughly constantly. The toxcast10 set shows a steady increase over the entire period 2000-2016; notably, toxcast100 and toxcast1000 show decreasing trends prior to January 2005, then increasing trends after this point.  Table \ref{tab.trends} confirms this visual impression, finding statistically significant differences in trends before and after January 2005 for the toxcast100 and toxcast1000 sets of terms.  

```{r monthly_scores_coeff, results='asis'}
## Regression slopes before and after the threshold date
trends = function (termset) {
	fit_before = lm(data = filter(scores_month, terms == termset, 
								  month < thresh_date, 
								  month != ymd('2000-02-01')),
			 score ~ month.rs)
	fit_after = lm(data = filter(scores_month, terms == termset, 
								 month >= thresh_date),
				   score ~ month.rs)
	return_df = data.frame(trend_before = fit_before$coefficients['month.rs'], 
						   trend_after = fit_after$coefficients['month.rs'],
						   trend_before_se = summary(fit_before)$coefficients['month.rs', 'Std. Error'],
						   trend_after_se = summary(fit_after)$coefficients['month.rs', 'Std. Error']) %>%
			mutate(z = (trend_after - trend_before)/sqrt(trend_after_se^2 + trend_before_se^2), 
				   ## Pass through the inline knit hook *now* in order to get 
				   ##  scientific notation consistent w/ rest of document
				   #p = knit_hooks$get('inline')(1 - pnorm(abs(z))))
				   p = 1 - pnorm(abs(z)))
	rownames(return_df) = termset
	return(return_df)
}

sapply(as.character(unique(scores_month$terms)), trends) %>% t %>%
	as.data.frame() %>%
	mutate(rownames = c('toxcast', 'toxcast10', 'toxcast100', 'toxcast1000')) %>%
	select(rownames, trend_before, trend_before_se, trend_after, trend_after_se, z, p) %>%
	setNames(c('', 
			   'trend (before)',
			   '(se)', 
		  		'trend (after)', 
			   	'(se)', 'z', 'p')) %>%
	xtable(#row.names = FALSE,
		  caption = 'Comparison of linear trends before and after January 2005. Trend values are annual changes in the number of articles per month.  z:  Z statistics for the difference in trends. p: p-values of z statistics against null hypotheses of no differences in trends. Articles from February 2000 are excluded from this analysis  \\label{tab.trends}',
		  digits = c(rep(1, 2), rep(1, 2), rep(2, 2), 2, 2),
		  display = c(rep('s', 2), rep('f', 4), 'f', 'e'),
		  format = 'latex')
```

Since scores are based on word frequencies, they are likely to be affected by the number of articles per month and the total length of those articles.  Figure \ref{fig.monthly_scores_control} plots scores after adjusting for these two factors.  After adjustment, "toxcast" shows a striking downward trend before 2005; the length of the articles were gradually changing over this period, and so the model expects to see an increase, but "toxcast" does not occur at all, so the trend line runs down.  The other three sets of terms exhibit roughly the same pattern, with flat trends before 2005 and modestly increasing trends afterwards.  This suggests, first, that the decreasing pre-2005 trends in figure \ref{fig.monthly_scores} are due to changes in the number of articles and total length; and second, that the increasing post-2005 trends are not only due to changes in the number of articles and total length.  

Aside of the general trends, the plots show a number of "peaks," months with especially high total scores.  Table \ref{tab.peaks} lists the articles with toxcast100 scores greater than 0 for those months with total toxcast100 scores at least 120 (excluding February 2000).  There are five such peaks, in October 2014, October 2015, November 2015, January 2016, and March 2016.  The October 2015 peak is essentially the result of a single article with an extremely high toxcast100 score; the other four peaks are due to combinations of multiple articles with modest to high scores.  

\clearpage
```{r peaks, results='asis'}
## Identify months with totals > 120
interesting_months = scores_month %>% filter(terms == 'toxcast100', 
											 score >= 120) %>%
	.[['month']]
## Extract the articles from those months
dataf %>% 
	filter(month %in% interesting_months, toxcast100 > 0, 
		   month != ymd('2000-02-01')) %>%
	select(month, date, title, score = toxcast100) %>%
	arrange(date) %>%
	mutate(month = as.character(month), 
		   date = as.character(date, format = '%B %e, %Y')) %>%
	split(as.character(.$month)) %>%
	lapply(function (x) {select(x, -month)}) %>% 
	xtableList(align = 'clp{4in}r', 
		  caption = 'Articles with non-zero toxcast100 scores, from months with total toxcast100 scores at least 120.  February 2000 is not included in this table.  \\label{tab.peaks}', digits = 0) %>%
	print(include.rownames=FALSE, comment=FALSE, 
		  tabular.environment='longtable', 
		  floating = FALSE)
```
\clearpage

The titles of the articles from these peaks indicate, first, that the toxcast100 score is a precise detector of CSS-relevant stories in BNA, with a low false positive rate (i.e., few articles with a high toxcast100 score but that are not relevant to CSS).  Second, the stories in these peaks are generally not about internal scientific developments, but instead about near- and medium-term regulatory uses of CSS tools.  However, third, stories that focus on specific hazards seem to have lower scores than stories that give broad overviews of high-throughput toxicology, at least within these peaks.  (Notably, the EDSP Pivot in June 2016 does not appear on this list; as far as I have been able to tell, BNA's only coverage of the EDSP Pivot was a brief note in one of the *Federal Register* overview articles.)  


### Sentiment Analysis ###

```{r sentiment-histograms, fig.width = 1.5*fig.width, fig.height = 1.5*fig.height, fig.cap = 'Kernel density estimates of length-normalized sentiment analysis scores.  Scores are on x-axis (note log scale), with densities on the y-axis. Vertical line indicates scores = 10 (1\\% of all article words). February 2000 is excluded from this plot. \\label{fig.sent_scores}'}
# Kernel density estimates of length-normalized sentiment analysis scores.  Scores are on x-axis (note log scale), with densities on the y-axis. Vertical line indicates scores = 10 (1\\% of all article words). February 2000 is excluded from this plot. \\label{fig.sent_scores}
ggplot(filter(toxcast_stories_m, month != ymd('2000-02-01')), 
	   aes(value.norm, fill = variable)) + 
	#geom_histogram(binwidth = 1) + 
	geom_density() +
	geom_vline(xintercept = 10) +
	#stat_ecdf() +
	facet_wrap(~ variable) + ylab('') + 
	scale_x_log10(name = '') + 
	scale_fill_discrete(guide = FALSE)
keep_sents = c('anticipation', 'fear', 'trust', 'negative', 'positive')
```

`r nrow(toxcast_stories)` stories had non-zero toxcast100 scores.  Figure \ref{fig.sent_scores} shows the distribution of sentiment analysis scores for each emotional valence across this set of stories, normalized by article length. Except for a substantial number of 0 scores, the distribution of scores for each valence is roughly log-Gaussian.  Anger, disgust, joy, sadness, and surprise have consistently low scores, so the analysis focuses on anticipation, fear, trust, negative, and positive.  

```{r sentiment-time, fig.width = 1.5*fig.width, fig.height = 1.5*fig.height, fig.cap = 'Distribution of sentiment analysis scores over time.  Black lines indicate loess regressions. February 2000 is excluded from this plot.  \\label{fig.sent_scores_time}'}
ggplot({toxcast_stories_m %>% 
		filter(variable %in% keep_sents, 
			   month != ymd('2000-02-01'))},
	   aes(date, value.norm, color = variable)) + 
	geom_point() + 
	geom_smooth(color = 'black', fill = 'grey') + 
	scale_color_discrete(guide = FALSE) +
	facet_wrap(~ variable) + scale_y_log10()
```

```{r sentiment-diff, fig.width = 1.5*fig.width, fig.cap = 'Distribution of differences between positive and negative emotional valence scores. \\textbf{A}: Density of differences. \\textbf{B}: Differences over time; red line is a loess regression.  February 2000 is excluded from these plots. \\label{fig.sen_dff}'}
toxcast_stories = toxcast_stories %>% mutate(diff = positive.x - negative.x)
sen_diff_density = ggplot(filter(toxcast_stories, month != ymd('2000-02-01')), 
						  aes(diff)) + 
						  	geom_density() + geom_rug() +
	xlab('positive - negative')
sen_diff_time = ggplot(filter(toxcast_stories, month != ymd('2000-02-01')), 
					   	aes(date, diff)) + geom_point(alpha = .25) +
	geom_smooth(color = 'red') + ylab('positive - negative')
plot_grid(sen_diff_density, sen_diff_time, labels = 'AUTO', hjust = 0)
```

Figure \ref{fig.sent_scores_time} shows the distribution of sentiment scores over time, along with local regressions.  All five emotional valences are basically stable over time.  Notably, positive scores appear to be higher on average than negative scores. Figure \ref{fig.sen_dff} shows the distribution of the difference between positive and negative scores.  `r (1 - ecdf(toxcast_stories$diff)(0)) * 100`% of articles have a greater positive than negative score, and this positive difference is stable over time. 

```{r, fig.width = 1.5*fig.width, fig.cap = 'Correlations between toxcast100 scores and (\\textbf{A}) positive and (\\textbf{B}) negative emotional valences. Blue lines are linear regressions. Note log scales on all axes. \\label{fig.toxcast_sent}'}
## No correlation between toxcast100 score and positive or negative
toxcast100_pos_plot = ggplot(filter(toxcast_stories, month != ymd('2000-02-01')),
	   aes(toxcast100, positive.x)) + geom_point() + geom_smooth(method = 'lm') +
	scale_x_log10() + scale_y_log10(name = 'positive')
toxcast100_pos_fit = lm(data = filter(toxcast_stories, month != ymd('2000-02-01')),
		log10(positive.x) ~ log10(toxcast100))
toxcast100_neg_plot = ggplot(filter(toxcast_stories, month != ymd('2000-02-01')),
	   aes(toxcast100, negative.x)) + geom_point() + geom_smooth(method = 'lm') +
	scale_x_log10() + scale_y_log10(name = 'negative')
toxcast100_neg_fit = lm(data = filter(toxcast_stories, month != ymd('2000-02-01'),
									  negative.x > 0),
		log10(negative.x) ~ log10(toxcast100))
plot_grid(toxcast100_pos_plot, toxcast100_neg_plot, nrow = 1, align = 'h', 
		  labels = 'AUTO', hjust = 0)
```

Figure \ref{fig.toxcast_sent} shows that there is no relationship between toxcast100 score and length-normalized positive scores.  There is a statistically significant ($p = `r summary(toxcast100_neg_fit)$coefficients[2,4]`$) negative association ($b = `r summary(toxcast100_neg_fit)$coefficients[2,1]`$) between toxcast100 score and negative valence; however, as indicated by the plot, this relationship is small and non-explanatory ($R^2 = `r summary(toxcast100_neg_fit)$r.squared`$). That is, coverage that is more focused on CSS-relevant research does not tend to be more or less positive, but does have a slight tendency to be less negative.  

```{r sentiment-diff-trust, fig.width = 1.5*fig.width, fig.cap = 'Distribution of differences between trust and fear emotional valence scores. \\textbf{A}: Density of differences. \\textbf{B}: Differences over time; red line is a loess regression.  February 2000 is excluded from these plots. \\label{fig.sen_diff_trust}'}
toxcast_stories = toxcast_stories %>% mutate(diff_trust = trust.x - fear.x)
sen_diff_density = ggplot(filter(toxcast_stories, month != ymd('2000-02-01')), 
						  aes(diff_trust)) + 
	geom_density() + geom_rug() +
	xlab('trust - fear')
sen_diff_time = ggplot(filter(toxcast_stories, month != ymd('2000-02-01')), 
					   aes(date, diff_trust)) + geom_point(alpha = .25) +
	geom_smooth(color = 'red') + ylab('trust - fear')
plot_grid(sen_diff_density, sen_diff_time, labels = 'AUTO', hjust = 0)
```

Figure \ref{fig.sent_scores_time} also suggests that "trust" scores are generally greater than "fear" scores.  Figure \ref{fig.sen_diff_trust} confirms this; trust scores are greater than fear scores for `r (1 - ecdf(toxcast_stories$diff_trust)(0)) * 100`% of articles.  

## Discussion ##

This study examined trade media coverage of CSS research using two text-mining techniques.  The first technique identified a set of standardized terms that appeared to be specific to CSS-relevant stories; this study showed increasing attention to CSS research since 2005, thereby addressing research question 1.  This change over time is due in part, though not entirely, to changes in the number of stories per month and the length of stories.  Coverage tends to be more relevant to CSS research when it deals with general or broad regulatory uses of this research in the short- and medium-term.  The second technique assessed the emotional valence of terms used in CSS-relevant stories, thereby addressing research question 2.  These stories were consistently more positive than negative, and expressed trust more than fear.  These patterns were consistent over time, and did not appear to be associated with the degree to which the article was CSS-relevant.  

The analysis in this study depends heavily on a substantial number of analytical decisions and assumptions.  Regarding the underlying data, key decisions include the terms used for the initial BNA search, and the inclusion/exclusion criteria used to refine the set of results.  Both of the subsequent analyses are based on simple word counts, which completely ignore structure and context; aggregate at the article or monthly level (rather than sentence, paragraph, quarter, or year); involve various decisions to use raw or normalized/adjusted scores or statistics; and use statistical techniques that assume Gaussian distributions and linearity.  For the "ToxCast" scores analysis, key decisions include decisions about how vocabulary would be standardized (which stopwords were removed, personal names were not identified and handled separately, common stems such as -s and -ing were not removed), the decision to use "toxcast" as the starting point for analysis, the use of Jaccard distance to identify associated terms, the size of the sets of associated terms (and the use of size, and rather than a distance threshold, to define these sets), and a choice of a date threshold.  The sentiment analysis depended heavily on a particular sentiment analysis tool, which in turn had its own substantive assumptions [@Mohammad2013]. 

While these decisions and assumptions do not invalidate the study's findings, it is highly plausible that they could have made a difference in the findings — that the findings could have been different if decisions had been made differently — which does complicate the interpretation of the findings.  This point is illustrated by the robustness analysis of the construction of the toxcast100 sets given in the appendix.  However, because of the large number of decisions and assumptions, it is impractical to check the robustness of these findings by surveying the entire space of possible analyses.  


## Appendix: Robustness Analysis of Four Term-Distance Metrics ##

This subsection examines concordance and discordance between four different ways of calculating distances from "toxcast" in the BNA vocabulary, and thus four different ways of determining which terms are included in the toxcast100 set.  In paticular, two different metrics are considered — Jaccard distance and $\ell_1$ distance — across every article in the dataset and excluding February 2000 articles.  

Jaccard distance is typically used to compare the distance between two sets $A$ and $B$.  In set-theoretic notation, Jaccard distance is defined as 
$$ d_J(A, B) = 1 - \frac{|A \cap B|}{|A \cup B|},$$
or 1 minus the fraction of elements that are in both sets.  Applied to the BNA vocabulary, a term's associated set is the set of all documents that use the term at least once.  Then, for two terms $A$ and $B$, their union $A \cup B$ is the set of all documents that use at least one of the two terms, and their intersection $A \cap B$ is the set of all documents that use both of the terms.  Note that this distance does not take into account how frequently a term occurs *within* a given document.  Jaccard distances range from 0 to 1, where 0 means the two terms occur in exactly the same documents and 1 means the terms never occur together.  

The $\ell_1$ distance takes within-document frequencies into account.  For a term $A$, let $a_i$ be the number of times it occurs in the $i$th document.  Then the $\ell_1$ distance is
$$ d_1(A, B) = \sum_i |a_i - b_i|, $$
or the sum of the absolute difference in frequencies across every document.  The minimum value of $\ell_1$ distance is 0, but unlike the Jaccard distance there is no maximum $\ell_1$ distance.  

Two terms can have a small Jaccard distance and a large $\ell_1$ distance if they occur in exactly the same documents (so that their intersections and unions are the same), but one term occurs much much more often than the other (so that the absolute differences in frequencies are large).  Two terms can have a small $\ell_1$ distance and a large Jaccard distance if they are both very rare (so that the absolute differences in frequencies are always 0 or small) but never occur together (so that their intersection is 0).  

The analysis in the main body of this study used Jaccard distance across the full set of articles (i.e., including February 2000) to define the toxcast10, toxcast100, and toxcast1000 sets.  To analyze the implications of this analytical choice, we calculate distances from "toxcast" across both the full set of articles and across a "restricted" set that excludes February 2000, using both Jaccard and $\ell_1$ metrics.  

```{r dist_robustness}
## Distance calculation robustness analysis
## Extract the toxcast row
toxcast = as.matrix(termdoc)['toxcast',]
## February 2000 articles
feb2000 = which(dataf$month == ymd('2000-02-01'))
#as.matrix(termdoc)[,-feb2000]

## Jaccard, full dataset
dist_Jf = apply(as.matrix(termdoc), 1,
		function (x) 1 - sum((x * toxcast) > 0) / 
	  					sum((x + toxcast) > 0))
## Jaccard, excluding Feb 2000
dist_Jx = apply(as.matrix(termdoc)[,-feb2000], 1,
	function (x) 1 - sum((x * toxcast[-feb2000]) > 0) / 
					sum((x + toxcast[-feb2000]) > 0))

## L1, full 
dist_1f = apply(as.matrix(termdoc), 1, function (x) sum(abs(x - toxcast)))
## L1, excluding Feb 2000
dist_1x = apply(as.matrix(termdoc)[,-feb2000], 1, 
				function (x) sum(abs(x - toxcast[-feb2000])))

## Combine the distances into one dataframe
dist_df = data.frame(jf = dist_Jf, jx = dist_Jx, l1f = dist_1f, l1x = dist_1x)
## Calculate rankings and filter to terms in at least one toxcast100
dist_df_r = dist_df %>% mutate_each(funs(min_rank)) %>% 
	filter(jf <= 100|jx <= 100|l1f <= 100|l1x <= 100) %>%
	setNames(c('Jaccard.full', 'Jaccard.restricted',
			   'L1.full', 'L1.restricted'))
```

```{r dist_pairs_plot, fig.height = 2 * fig.height, fig.width = 2 * fig.width, fig.cap = 'Pairs plot for four ways of calculating rankings/distances from "toxcast" in the BNA vocabulary, for terms included in at least one of the four toxcast100 sets.  \\textbf{Main diagonal}: X-axis values are rankings, with 1 = "toxcast".  Curves are kernel density estimates of distribution of ranking values.  \\textbf{Lower triangle}: Scatterplots of ranking values across pairs of distance calculations; both axis are rankings, with 1 = "toxcast".  Black lines are linear regressions.  \\textbf{Upper triangle}: Pearson correlation coefficients on pairs of rankings, equivalent to Spearman rank correlation coefficients on distance calculations. \\label{fig.dist_pairs}', fig.keep = 'last'}
## Pairs plot of distance values across every term
#ggscatmat(dist_df) + theme_gray() + ylab('') + xlab('')
## Ranked and filtered
dist_pairs_plot = ggpairs(dist_df_r, 
		lower = list(continuous = wrap('smooth', color = 'blue')),
		upper = list(continuous = wrap('cor', color = 'black')),
		columnLabels = c('Jaccard\n(full)', 'Jaccard\n(restricted)',
						TeX('l_1 (full)'),
						TeX('l_1 (restricted)'))
	) + theme(panel.background = element_rect(fill = 'grey95'),
			  panel.grid = element_blank())
dist_pairs_plot[3,1] = dist_pairs_plot[3,1] + 
	coord_cartesian(ylim = c(0, max(dist_df_r$L1.full)))
dist_pairs_plot[3,2] = dist_pairs_plot[3,2] + 
	coord_cartesian(ylim = c(0, max(dist_df_r$L1.full)))
dist_pairs_plot[4,1] = dist_pairs_plot[4,1] + 
	coord_cartesian(ylim = c(0, max(dist_df_r$L1.restricted)))
dist_pairs_plot[4,2] = dist_pairs_plot[4,2] + 
	coord_cartesian(ylim = c(0, max(dist_df_r$L1.restricted)))

## Rotate y-axis labels horizontally
## after http://stackoverflow.com/questions/28427572/manipulating-axis-titles-in-ggpairs-ggally
print(dist_pairs_plot, left = 1, bottom = .75)
g <- grid::grid.ls(print=FALSE)
idx <- g$name[grep("text", g$name)]
for (i in idx[1:4]) {
	grid::grid.edit(grid::gPath(i), rot = 0, hjust = 0, gp = grid::gpar(cex = .75))
}
for (i in idx[5:8]) {
	grid::grid.edit(grid::gPath(i), vjust = -.25, gp = grid::gpar(cex = .75))
}
```

Figure \ref{fig.dist_pairs} shows scatterplots and correlation coefficients for each pair of these distance calculations, for `r nrow(dist_df_r)` terms that are included in each at least one toxcast100 set.  The plot indicates strong aggreement between full and restricted calculations, but strong disagreement between Jaccard and $\ell_1$ distances.  Similarly, figure \ref{fig.incl_pairs} shows scatterplots and correlation coefficients for inclusion in the toxcast100 sets.  There is strong agreement between full and restricted calculations using the same metric, but almost complete disagreement between the two metrics, i.e., the two metrics produced almost completely different toxcast100 sets.  

```{r incl_pairs_plot, fig.height = 2 * fig.height, fig.width = 2 * fig.width, fig.cap = 'Pairs plot for four ways of determining membership in toxcast100 sets.  \\textbf{Lower triangle}: Inclusion/exclusion size plots.  Size of rectangles corresponds to number of terms in each cell of the subplot.  Upper-left cell corresponds to terms in both toxcast100 sets; upper-right cell corresponds to terms in the y-axis set but not the x-axis set; and so on.  \\textbf{Upper triangle}: Pearson correlation coefficient for set membership, taking inclusion = 1, exclusion = 2. \\label{fig.incl_pairs}', fig.keep = 'last' }
## Inclusion/exclusion for toxcast100 sets
dist_df_inex = dist_df_r %>% mutate_each(funs(. <= 100)) %>% 
	mutate_each(funs(factor(., levels = c('TRUE', 'FALSE'))))
my_custom_cor = function(data, mapping, color = I("black"), ...) {
	x = eval(mapping$x, data) %>% as.numeric()
	y = eval(mapping$y, data) %>% as.numeric()
	ct = cor(x, y)
	## Use Cramér's V
	#ct = lsr::cramersV(x, y)
	
	ggally_text(
		label = paste('Corr:', format(ct, digits = 3), sep = '\n'),
		mapping = aes(),
		xP = 0.5, yP = 0.5,
		color = color,
		...
	)
}
incl_pairs_plot = ggpairs(dist_df_inex, 
		columnLabels = c('Jaccard\n(full)', 'Jaccard\n(restricted)',
						TeX('l_1 (full)'),
						TeX('l_1 (restricted)')),
		#axisLabels = 'internal',
		lower = list(
			'discrete' = 'ratio'
			), 
		upper = list(
			#'discrete' = 'blank'
			'discrete' = my_custom_cor
			)
		, diag = list('discrete' = 'blankDiag')
	) + theme(panel.background = element_rect(fill = 'grey95'), 
			  panel.grid = element_blank())

## Rotate y-axis labels horizontally
## after http://stackoverflow.com/questions/28427572/manipulating-axis-titles-in-ggpairs-ggally
print(incl_pairs_plot, left = .75, bottom = .75)
g <- grid::grid.ls(print=FALSE)
idx <- g$name[grep("text", g$name)]
for (i in idx[1:4]) {
	grid::grid.edit(grid::gPath(i), rot = 0, hjust = 0, gp = grid::gpar(cex = .75))
}
for (i in idx[5:8]) {
	grid::grid.edit(grid::gPath(i), vjust = -.5, gp = grid::gpar(cex = .75))
}
```

Decisions about which metric to use had a substantial downstream effect on the findings of this study.  An early version of this study used the $\ell_1$ distance, rather than Jaccard distance.  The former norm generates a list of toxcast100 terms that includes several obvious typos, such as "dataespecially," suggesting that the construction was substantially picking other rare terms, rather than terms that were actually semantically related to "toxcast."  As indicated by table \ref{tab.toxcast100}, Jaccard distance produces a much more meaningful set of terms.  Using the $\ell_1$ distance, the especially high-scoring months were January 2009, March 2011, March 2012, April 2014, and October 2014.  Except for October 2014, none of these months were especially high-scoring using the Jaccard metric.  Among other things, a story from March 2012 about a partnership between EPA and L'Oréal was prominent when the $\ell_1$ distance was used, but no longer prominent once Jaccard distance was adopted instead.  Thus, a subtle analytical decision — which mathematical function to use to calculate distances between pairs of terms — can have substantial downstream effects.  

All together, the toxcast100 term lists cannot be considered robust.  Their construction, use, and interpretation should be tailored carefully to the particular aims of any given analysis.  

